# KOR 
# ENG
(Slide 1: Title) 안녕하십니까. 오늘 발표할 주제는 "NiceSched: 계층화된 메모리 환경에서 메모리 접근 지역성을 인지하는 동적 Nice 스케줄링"입니다. 


(Slide 2~4: Background) 먼저 연구의 배경입니다. 최근 데이터센터 운영에 있어 가장 큰 이슈 중 하나는 비용입니다. 지난 18개월간의 지표를 보면 DRAM 가격은 약 2.5배까지 급등했습니다. 전체 데이터센터 비용(TCO)의 약 33%가 메모리에서 발생한다는 점을 고려할 때, 이는 매우 큰 부담입니다. 이러한 문제를 해결하기 위해 저렴한 비용으로 메모리 용량을 확장할 수 있는 CXL(Compute Express Link) 기술이 등장했습니다. 
하지만 CXL 역시 기존 메모리 구조와 마찬가지로 '메모리 대역폭'과 '지연 시간(Latency)'이라는 물리적 한계, 즉 'Memory Wall' 문제를 여전히 안고 있습니다. 따라서 메모리 대역폭과 지연 시간 둘 다 고려해야하는 상황입니다.


기존 계층화된 메모리 관리 연구들은 주로 모니터링을 통해 페이지를 분류하고 재배치하는 기법들을 시도했습니다. 그림은 높은 계층 메모리에 많은 접근을 하는 핫한 페이지 덜 접근하는 콜드 페이지를 분류하는 방식을 보여줍니다. 높은 계층 메모리에는 핫 페이지를 , 낮은 계층에는 콜드 페이지를 두기위해 페이지 마이그레이션 하는 것이 이전 연구들의 핵심 방법론입니다.


(Slide 5~9: Challenges & Motivation) 
현재 보시는 표는 선행 연구들이 사용한 모니터링의 종류와 모니터링의 기본 페이지 사이즈, 모니터링 주기, 모니터링 정확성을 상대적으로 비교한 것입니다.


살펴보면, 5개의 이전 연구들은 각기 다른 한계와 장점을 지니고 있습니다.


첫째, 하드웨어 이벤트(HW-event) 방식은 정확도는 높지만, 모니터링 대상이 늘어날수록 오버헤드가 급격히 커지고 하드웨어 벤더 간 호환성이 떨어진다는 단점이 있습니다. 
둘째, 현재 기본 정책으로 사용되는 페이지 폴트(Page fault) 방식은 시스템에 가해지는 오버헤드가 가장 크다는 것이 치명적입니다. 하지만 AutoNUMA와 결합된다는 장점이 존재합니다.
셋째, PTE-scan 방식은 스캔 빈도에 따라 정확도와 오버헤드가 극단적으로 갈리기 때문에 적절한 빈도를 튜닝하기가 매우 까다롭습니다.


또한, 마이그레이션 오버헤드를 줄이기 위해 Huge page를 도입하기도 하지만, 일부 영역만 핫(Hot)해도 전체를 핫하다고 오판하는 'Hot Bloat' 문제를 야기합니다. Huge Page 내 불균형은  성능 하락을 초래합니다. 


요약하자면, 하드웨어 이벤트 방식은 높은 정확도를 보이지만 안정성이 떨어지고,
페이지 폴트 방식은 높은 오버헤드를 보이지만 Numa balancing의 기본 정책이기에 높은 이식성을 지닙니다. 마지막으로 PTE-scan 방식은 스캔 주기를 결정하기 까다롭지만 높은 정확성을 보입니다.


하지만 무엇보다 기존 연구들이 가진 공통적이고 근본적인 문제는, 페이지를 Hot/Cold로 나누고 물리적으로 이동시키는 '페이지 마이그레이션' 자체에 추가적인 컴퓨팅 자원을 소모하고 대역폭을 소모한다는 점입니다.
결국 오버헤드와 정확도 사이의 트레이드오프 속에서, 현재 "그 누구도 완벽한 승자는 없는(No one is winner)" 상황입니다. 이 점은 새로운 접근 방식이 필요하다는 것을 의미합니다.


그래서 저희는 비용과 대역폭을 소모시키는 마이그레이션 없이 tiered memory를 활용하기 위해 스케줄링을 사용하였습니다. 






(Slide 10~11: Objective & Design)


기존 선행 연구들의 주요 목표는 원격 접근(Remote access)을 줄이는 것, 고계층 메모리를 효율적으로 활용하는 것, 그리고 모니터링 오버헤드를 줄이는 것이었습니다. 저희 연구는 이 목표를 달성하기 위해 새로운 접근 방식인 NiceSched를 사용합니다.
 저희가 제안하는 NiceSched의 핵심 아이디어는 간단합니다. 운영체제의 기본 스케줄링 메커니즘은 NICE 값을 사용하여 CPU 실행 시간을 제어합니다. 
저희는 이 NICE 메커니즘을 활용해 CPU시간을 점유하고, 고계층 메모리를 잘 활용하는 프로세스를 우대하여 전체적인 시스템 향상을 끌어내는 것 입니다. 스케줄링을 활용하면 마이그레이션, 추가적인 모니터링 없이 티어링 환경을 잘 활용할 수 있습니다.


설계 구조를 보시겠습니다. (슬라이드 11의 다이어그램을 가리키며) 
전체적인 구조는 AutoNUMA 아래에서 완벽하게 통합되어 동작합니다. 저희는 Autonuma가 주기적으로 수집하는 정보를 활용하여 높은 계층 메모리의 locality가 높은 프로세스에게 높은 우선순위를 주어 스케줄링을 제어합니다.


p.16
AutoNuma는 주기적으로 프로세스의 VMA를 스캔하여 LR_ratio를 계산합니다. 수식은 전체 메모리 접근 중 '고계층 메모리(High tier)'에 대한 접근 비율을 뜻합니다. 높은 lr_ratio는 high tier memory에 대한 높은 지역성을 뜻합니다. 반면 낮은 lr_ratio는 낮은 지역성을 뜻합니다.


p.17
그림은 lr_ratio를 직관적으로 확인할 수 있습니다.  높은 계층의 메모리에 많은 접근을 하는 프로세스 P4는 high-tier memory access가 증가하게 되고 이는 높은 lr_ratio, 높은 지역성을 갖게 됩니다. NiceSched는 이러한 프로세스에 높은 우선순위를 부여하게 됩니다.


p.18
우선순위 nice는 lr_ratio가 0-5까지 0 을 가지게 되고 그 이상부터 점차 낮아져 최소 -19의 nice값을 가지게 됩니다. please remind, nice값은 낮을 수록 더 많은 cpu cycle을 부여받게 됩니다.
따라서, 높은 계층 메모리의 지역성이 높은 프로세스는 낮은 NICE 값을 받아 CPU를 더 많이 할당받습니다. 


저희는 오로지 5이상의 lr_ratio, 즉, 높은 지역성을 갖는 프로세스에게 nice값을 부여합니다. 그 이유는, side effect를 피하고 우선순위의 상대성과 지역성을 중심으로 우선순위를 부여했기 때문입니다. 자세한 내용은 논문을 참조하기 바랍니다.




(슬라이드 14의 큐 그림을 가리키며) 계산된 우선순위는 즉시 프로세스의 우선순위, 즉 NICE 우선순위 값에 반영됩니다. 결과적으로 nice 값이 설정되면 CPU Run Queue 상에서 지역성이 높은 프로세스 P4가 우선적으로 할당이 되려고 하고, 그 이후 더 많은 CPU 사이클을 점유하게 됩니다. 
이러한 동작 과정은 high tier 메모리 접근 효율성이 향상되도록 야기합니다.


이 그림은 실험적으로 nice값의 영향을 보여줍니다. x축은 cpu 점유 시간 y축은 CPU넘버입니다. 
우리는 nice의 영향을 파악하기 위해 7z워크로드를 5번 실행하고 그 중 하나의 프로세스에게 nice -19를 부여했습니다. 그 후, perf-trace-viewer를 통해 각 프로세스의 cpu 점유 시간을 측정하였습니다. 아래 그림은 nice의 영향을 확인할 수 있으며, nice를 부여하지 않은 위의 그림보다 더 촘촘하게 cpu cycle을 부여받아 약 77% 빠르게 종료됨을 보여줍니다. 이는 nice값의 실용성을 보여줍니다.


(Slide 18~21: Evaluation Setup) 평가 부분입니다. 저희는 실제 하드웨어 제약상, 2개의 NUMA 노드 중 한 쪽의 CPU를 끄는 방식으로 CXL 환경을 모사했습니다. 또한 메모리의 비율을 구성하기위해 높은 계층의 메모리를 제약하는 형식으로 비율을 모사하였습니다.




저희는 NiceSched의 효과를 다음 세 가지 기준으로 평가했습니다:


1. 변화하는 high tier memory 대 low tier memory 비율 하에서 어떻게 작동하는가?
2. 스케줄링 조정이 얼마나 효과적인가?
3. 저계층 메모리 접근이 얼마나 감소했는가?
워크로드로는 NPB, XSBench, Graph Processing(PageRank 등)을 사용했습니다 .
(슬라이드 18의 히트맵을 보시겠습니다) 사전 분석 결과, BT나 LU 같은 워크로드는 메모리 접근이 특정 영역에 집중되는(Locality) 특성을 보인 반면, MG나 UA는 메모리를 산발적으로 사용하는 패턴을 보였습니다. 저희 방식은 이 '지역성'이 뚜렷할수록 더 큰 효과를 발휘합니다.


(Slide 22~23: Results - Performance) 성능 평가 결과입니다. 
저희는 low-tier memory가 많아져가는 상황을 가정하여 실험을 진행하였습니다. x축은 high-tier memory와 low-tier memory의 비율을 의미하고 오른쪽으로 갈수록 low-tier memory가 많아짐을 의미합니다. y축은 vanilla kernel으로 정규화된 성능을 뜻합니다. 지역성이 뚜렷한 BT, LU 워크로드는 low-tier memory가 많아지는 상황 즉 low-tier memory access가 많아지는 상황에서 더 큰 성능 향상을 보이고 지역성이 뚜렷하지 않은 워크로드는 성능의 경향성이 줄어듦을 보입니다. 또한 CG 워크로드 지역성이 높은 반면 low-tier memory ratio가 높아질 수록 성능 향상 폭이 줄어드는 것을 확인할 수 있습니다. 이는 high-tier memory를 제약함으로서 low-tier memory ratio를 높인 side effect이며 high-tier memory보다 워크로드의 사이즈가 더 크기 때문에 나오는 결과입니다. 따라서 저희는 지역성이 높은 워크로드에 대해 이득이 높다는 것을 확인했습니다. 이에 원격 메모리 접근을 줄임으로써 최대 13%의 성능 향상을 달성했습니다. 물론 오버헤드로 인해 성능이 소폭 감소한 경우도 있었으나, 그 폭은 1% 미만으로 미미했습니다. 이는 저희 방식이 시스템에 주는 부담이 매우 적다는 것을 증명합니다.
 
다음은 low tier memory access에 대한 그래프입니다. 위의 그림은 7z워크로드에 대해 vanilla kernel 대비 low tier memory access얼마나 효과적으로 줄였는가를 보여줍니다. 그림의 오른쪽으로 갈수록 low-tier memory ratio가 높아짐을 의미합니다. 전체적으로 약 2배 가량 low-tier memory의 접근을 줄일 수 있음을 확인하였습니다. 이는 NiceSched가 migration없이 효율적으로 low tier memory access를 줄일 수 있다는 것을 보여줍니다. 반면 low-tier memory가 커질수록 low-tier memory access가 많아지는 것은 전에 설명했듯이 high tier memory가 줄어들기 때문입니다.


또한, 아래 그림은 같은 워크로드를 시간에 따라 low tier memory의 접근 횟수를 기록한 것입니다. x축은 시간을 의미하고 y축은 low-tier memory access횟수를 의미합니다. 파란색 선인 vanilla kernel보다 더 낮은 low tier memory access와 더 빠르게 스케줄링 하는 것을 볼 수 있습니다.  즉, NiceSched는 스케줄링을 이용하여 워크로드의 사용 시간을 앞당기고 high-tier memory를 잘 사용하는 워크로드를 우대하기 때문에 더 낮은 low tier memory access를 기록하고 high tier memory를 더 잘 사용함을 보여줍니다.


(Slide 25~26: Conclusion) 
결론입니다. NiceSched는 복잡한 페이지 이동이나 Hot/Cold 분류 없이, 메모리 접근 지역성을 인지하는 스케줄링만으로 계층화된 메모리 시스템의 효율을 높였습니다. 
비록 시뮬레이션 환경이라는 한계와 가존 연구와의 비교의 부재, 제약적인 실험환경이라는 한계가 존재하지만,
향후에는 기존의 페이지 마이그레이션 기법(Memtis 등)과 저희 스케줄링 기법을 결합하거나 , 3계층 이상의 다중 티어 환경으로 확장하여 연구를 발전시킬 계획입니다.


(Last Slide) 이상으로 발표를 마치겠습니다. 경청해 주셔서 감사합니다. 질문 있으시면 답변 드리겠습니다.


________________


ENG
________________




안녕하세요. 오늘 발표드릴 내용은 "NiceSched: 계층화된 메모리 환경에서 메모리 접근 지역성을 인지하는 동적 nice 스케줄링(memory access locality aware dynamic nice scheduling in tiered memory)" 입니다.


배경 (Background)
먼저 배경에 대해 말씀드리겠습니다.
최근 몇 년간 메모리 가격은 크게 상승했습니다. 지난 18개월 동안 DDR5-6000 2x32GB의 평균 USD 가격을 살펴보면, 오늘날 DRAM 가격은 약 2.5배까지 증가했습니다. 이러한 비용 문제를 해결하기 위해 CXL(Compute Express Link)이 등장하여 총 소유 비용(TCO)을 줄이는 데 기여하고 있습니다. 데이터센터 총 비용의 33%가 메모리에 의존하고 있기 때문에, CXL의 등장으로 데이터센터는 스케일 업을 위해 새로운 메모리를 구매할 필요가 없어지게 될 수도 있습니다.


하지만 메모리 벽은 여전히 중요한 문제입니다. 효율성을 해결하는 핵심은 메모리 대역폭이지만, CXL 역시 동일한 어려움(challenge)을 가지고 있습니다.


도전 과제 (Challenges)
계층화된 메모리 환경에서 메모리를 관리하는 데 모니터링 방식을 주로 사용하는데 이는 여러 도전 과제가 있습니다.
1. 하드웨어 이벤트(HW-event): 모니터링해야 할 페이지가 많아질수록 모니터링 오버헤드도 증가합니다. 또한, CPU 연결 및 CXL 연결 모두에서 안정적이지 않으며 모든 하드웨어 벤더를 지원하지 못하는 문제가 있습니다.
2. 페이지 폴트(Page fault): 페이지 폴트는 높은 오버헤드를 유발하는 것으로 알려져 있지만, 현재 기본 정책으로 사용되고 있습니다.
3. PTE-scan: 이 방법은 모니터링 빈도를 결정하기 어렵다는 문제가 있습니다.
4. Huge page 사용 시: Huge page를 사용할 경우 "Hot Bloat" 이슈가 발생할 수 있습니다.
또한 
선행 연구 분석 및 동기 (Prior Work and Motivation): 
기존 선행 연구들(TPP, MEMTIS, M5, FlexMem, ADT)은 페이지 폴트, HW-event, PTE-scan 등의 메커니즘을 사용하며, 각각 기본 페이지 크기, 재현성(Recency), 정확도(Accuracy)에서 상이한 특징을 보입니다. 


많은 연구들이 페이지 마이그레이션과 페이지를 HOT과 COLD를 구분하는데 많은 컴퓨팅을 사용합니다. 
이는 memory wall에 제약되며 쓸데없는 컴퓨팅을 낭비합니다.
현재로서는 "승자는 없다(No one is winner)"는 결론에 도달했으며, 
이를 사용하지 않는 다른 접근 방식이 필요했습니다.


기존 선행 연구들의 주요 목표는 원격 접근(Remote access)을 줄이는 것, 고계층 메모리를 효율적으로 활용하는 것, 그리고 모니터링 오버헤드를 줄이는 것이었습니다. 저희 NiceSched는 이 목표를 달성하기 위해 새로운 접근 방식을 제안합니다.


NiceSched 설계 (NiceSched Design)
저희는 스케줄링 메커니즘에 초점을 맞추었습니다. CFS(Completely Fair Scheduler)와 EEVDF 같은 스케줄러는 NICE 값을 사용하여 CPU 실행 시간을 제어합니다. 저희는 이 NICE 메커니즘을 고계층 메모리를 극도로 활용하도록 이용합니다.
LR_ratio 계산: NiceSched는 NUMA 모니터링을 사용하여 LR_ratio를 계산합니다.
​
ha 는 고계층 메모리 접근(high tier memory access)을 의미합니다.​
la 는 저계층 메모리 접근(low tier memory access)을 의미합니다.
• LR_ratio=ha/(ha+la)*10 입니다.
우선순위 매핑 및 동적 NICE 조정: 
이 LR_ratio를 바탕으로 **우선순위 매핑(Priority Mapping)**을 수행하여 NICE 값을 결정합니다.
• nice=min(0,20−lr_ratio×3.9)
• NICE 값의 범위는 (-20, 20]이며, 값이 낮을수록 더 많은 CPU 사이클을 얻게 됩니다.
이러한 *
*동적 NICE 조정(Dynamic Nice Adjustment)**을
 통해 매 numa motoring 마다 우선순위 매핑 및 NICE 설정을 스케줄링에 반영합니다. 이는 시스템에 반영되어 프로세스가 더 많은 CPU 사이클을 얻게 하는 효과를 가져옵니다.


평가 (Evaluation)
저희는 NUMA 노드 2개 환경에서 NUMA 노드 1번의 CPU를 끔으로서 모의 CXL 환경을 구성했습니다.
또한, high tier의 메모리를 제약함으로서 메모리 비율을 측정했습니다.


평가 기준 (Criteria): 
저희는 NiceSched의 효과를 다음 세 가지 기준으로 평가했습니다:


1. 변화하는 고계층 대 저계층 메모리 비율 하에서 작동할 수 있는가?
2. 스케줄링 조정이 얼마나 효과적인가?
3. 저계층 메모리 접근이 얼마나 감소했는가?




작업 부하로는 메모리 및 CPU 집약적인 NPB (MG, CG, BT, LT, UA), XSBench, 7zip, 그리고 그래프 처리 워크로드인 SSSP, Page Rank를 사용했습니다.
memory access heatmap을 분석한 결과 BT, CG 워크로드는 메모리 지엽적이고
MG, UA 워크로드는 메모리를 광범위적으로 사용함을 알 수 있습니다.
이에따라 메모리 지엽적인 워크로드의 성능 향상이 도드라지고, 광범위적인 메모리 사용량을 가지는 워크로드는 성능 향상이 적은 모습을 관찰했습니다. 
NiceSched는 원격 접근(remote access) 감소 덕분에 **최대 13%의 성능 향상(improvement)**을 보였습니다. 하지만, 이점을 얻지 못하고 오버헤드만 발생하는 경우에는 **1% 미만의 성능 감소(decrease)**가 관찰되기도 했습니다.
결론적으로, NiceSched는
• 저계층 메모리 접근을 효율적으로 줄여 약 2배 이상 줄이고, 고계층 메모리를 효율적으로 활용하는 것으로 나타났습니다. 이는 아래 그림의 메모리 접근 패턴이 왼쪽으로 shiting 되는 모습으로 확인할 수 있습니다.
요약하면, 저희는 티어링 메모리의 새로운 접근 방식을 제안했고
페이지 마이그레이션이나 구분없이 효율적인 low tier memory를 줄이고
high tier memory를 효율적으로 활용했습니다.
반면에 이전 연구와의 비교나 실제 하드웨어는 저희의 한계이며 실험 환경을 많이 제약했다는 한계도 존재합니다.
마지막으로 저희는 앞으로 이전 연구의 방법을 혼합하여 새로운 프레임워크를 만들 것이며,  3 계층 그 이상까지의 티어링 메모리에 대해서도 지원할 예정입니다.
질의응답 (QnA)
이상으로 발표를 마치겠습니다. 질문 있으시면 해 주십시오.